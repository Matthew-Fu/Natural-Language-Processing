{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of LanguageModeling.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "moWB9udaKesP"
      },
      "source": [
        "Your task is to train *character-level* language models. \n",
        "You will train unigram, bigram, and trigram character-level models on a collection of books from Project Gutenberg. You will then use these trained English language models to distinguish English documents from Brazilian Portuguese documents in the test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gHFJmuftHJld",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4fbc33ea-a133-4500-f323-fb4b25ae92b0"
      },
      "source": [
        "import pandas as pd\n",
        "import httpimport\n",
        "import numpy as np\n",
        "import math\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "with httpimport.remote_repo(['lm_helper'], 'https://raw.githubusercontent.com/jasoriya/CS6120-PS2-support/master/utils/'):\n",
        "  from lm_helper import get_train_data, get_test_data"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/brown.zip.\n",
            "[nltk_data] Downloading package mac_morpho to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/mac_morpho.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8U0UCuyHQkai"
      },
      "source": [
        "This code loads the training and test data. Each dataset is a list of books. Each book contains a list of sentences, and each sentence contains a list of words. For building a character language model, you should join the words of a sentence together with a space character."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6x0pfuiEChTh"
      },
      "source": [
        "# get the train and test data\n",
        "train = get_train_data()\n",
        "test, test_files = get_test_data()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n-tP4DlcTxMc"
      },
      "source": [
        "# join the words of a sentence together with a space character\r\n",
        "def combine(book):\r\n",
        "  combine_sentence = []\r\n",
        "  for sentence in book:\r\n",
        "    combine_sentence.append(\" \".join(sentence).strip())\r\n",
        "  return combine_sentence\r\n",
        "\r\n",
        "train_list = []\r\n",
        "for book in train:\r\n",
        "  train_list.append(combine(book))\r\n",
        "\r\n",
        "test_list = []\r\n",
        "for book in test:\r\n",
        "  test_list.append(combine(book))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_WAO9VjFLArq"
      },
      "source": [
        "## 1.1\n",
        "Collect statistics on the unigram, bigram, and trigram character counts.\n",
        "\n",
        "If your machine takes a long time to perform this computation, you may save these counts to files in your github repository and load them on request. This is not necessary, however."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oh4VOoiSIoUF",
        "cellView": "code"
      },
      "source": [
        "#@title Default title text\n",
        "# Your code here\n",
        "train_set = []\n",
        "for book in train_list:\n",
        "  train_set.append(\" \".join(book))\n",
        "\n",
        "test_set = []\n",
        "for book in test_list:\n",
        "  test_set.append(\" \".join(book))\n",
        "\n",
        "train_total = [[\"<s>\", \"<s>\"] + list(sent) + [\"</s>\", \"</s>\"] for sent in train_set]\n",
        "\n",
        "unigram_dict = {}\n",
        "bigram_dict = {}\n",
        "trigram_dict = {}\n",
        "\n",
        "for document in train_total:\n",
        "  for ch in document:\n",
        "    if ch in unigram_dict:\n",
        "      unigram_dict[ch] += 1\n",
        "    else:\n",
        "      unigram_dict[ch] = 1\n",
        "  for i in range(1, len(document)):\n",
        "    if tuple(document[i-1:i+1]) in bigram_dict:\n",
        "      bigram_dict[tuple(document[i-1:i+1])] += 1\n",
        "    else:\n",
        "      bigram_dict[tuple(document[i-1:i+1])] = 1\n",
        "  for i in range(2, len(document)):\n",
        "    if tuple(document[i-2:i+1]) in trigram_dict:\n",
        "      trigram_dict[tuple(document[i-2:i+1])] += 1\n",
        "    else:\n",
        "      trigram_dict[tuple(document[i-2:i+1])] = 1\n",
        "\n",
        "dict = unigram_dict\n",
        "for k,v in bigram_dict.items():\n",
        "  dict[k] = v\n",
        "for k,v in trigram_dict.items():\n",
        "  dict[k] = v"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RS3mnaIvQnhI"
      },
      "source": [
        "## 1.2\n",
        "Calculate the perplexity for each document in the test set using the linear interpolation smoothing method. For determining λs for linear interpolation of the trigram, bigram, and unigram models, you can divide the training data into a new training set (80%) and a held-out set (20%).\n",
        "Then choose ~10 random pairs of $(\\lambda_3, \\lambda_2)$ such that $1 > \\lambda_3 > \\lambda_2 > 0$ and $\\sum_{i=1}^3 \\lambda_i = 1$ to test on held-out data.\n",
        "\n",
        "Some documents in the test set are in Brazilian Portuguese. Identify them as follows: \n",
        "  - Sort by perplexity and set a cut-off threshold. All the documents above this threshold score should be categorized as Brazilian Portuguese. \n",
        "  - Print the file names (from `test_files`) and perplexities of the documents above the threshold\n",
        "\n",
        "    ```\n",
        "        file name, score\n",
        "        file name, score\n",
        "        . . .\n",
        "        file name, score\n",
        "    ```\n",
        "\n",
        "  - Copy this list of filenames and manually annotate them as being correctly or incorrectly labeled as Portuguese.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QQF4HhQGOZD8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6aa7901d-7803-4261-d258-de0ec6a26c3c"
      },
      "source": [
        "# Your code here\r\n",
        "# divide the training data into a new training set (80%) and a held-out set (20%)\r\n",
        "new_training_set, held_out_set = train_test_split(train_set, train_size = 0.8)\r\n",
        "\r\n",
        "# choose ~10 random pairs of  (λ3,λ2)  such that  1>λ3>λ2>0  and  ∑λi=1\r\n",
        "def choose_random():\r\n",
        "  a, b, c = np.random.rand(3)\r\n",
        "  lambda3 = max(a,b,c)/(a+b+c)\r\n",
        "  lambda2 = min(a,b,c)/(a+b+c)\r\n",
        "  return lambda3, lambda2\r\n",
        "\r\n",
        "lambda_list = []\r\n",
        "for i in range(10):\r\n",
        "  lambda_list.append(choose_random())\r\n",
        "\r\n",
        "# test on held-out data\r\n",
        "held_out = [[\"<s>\", \"<s>\"] + list(sent) + [\"</s>\", \"</s>\"] for sent in held_out_set]\r\n",
        "held_out_total = []\r\n",
        "for document in held_out:\r\n",
        "  held_out_total.extend(document)\r\n",
        "\r\n",
        "V = sum(len(train_total[i]) for i in range(len(train_total)))\r\n",
        "\r\n",
        "p_list = []\r\n",
        "for lambda3, lambda2 in lambda_list:\r\n",
        "  lambda1 = 1 - lambda3 - lambda2\r\n",
        "  p_sum = {}\r\n",
        "  for k, v in trigram_dict.items():\r\n",
        "    wn_2, wn_1, wn = k\r\n",
        "    p_tri = v / bigram_dict[(wn_2, wn_1)]\r\n",
        "    p_bi = bigram_dict[(wn_1, wn)] / unigram_dict[wn_1]\r\n",
        "    p_uni = unigram_dict[wn] / V\r\n",
        "    p_sum[k] = lambda3 * p_tri + lambda2 * p_bi + lambda1 * p_uni\r\n",
        "  p_list.append(p_sum)\r\n",
        "\r\n",
        "ppl = []\r\n",
        "for i in range(len(p_list)):\r\n",
        "  d = p_list[i]\r\n",
        "  M = len(held_out_total)\r\n",
        "  loss = 0\r\n",
        "  for j in range(2, M):\r\n",
        "    if tuple(held_out_total[j-2:j+1]) in d:\r\n",
        "      loss -= math.log(d[tuple(held_out_total[j-2:j+1])], 2) \r\n",
        "  ppl.append(2**(loss/M-2))\r\n",
        "\r\n",
        "# determining λs\r\n",
        "num = ppl.index(min(ppl))\r\n",
        "best_lambda = lambda_list[ppl.index(min(ppl))]\r\n",
        "\r\n",
        "# Calculate the perplexity for each document in the test set using the linear interpolation smoothing method\r\n",
        "lambda3, lambda2 = best_lambda\r\n",
        "score = []\r\n",
        "test_doc = []\r\n",
        "for document in test_set:\r\n",
        "  test_doc.append([\"<s>\", \"<s>\"] + list(document) + [\"</s>\", \"</s>\"])\r\n",
        "\r\n",
        "for doc in test_doc:\r\n",
        "  d = p_list[num]\r\n",
        "  M = len(doc)\r\n",
        "  loss = 0\r\n",
        "  for j in range(2, M):\r\n",
        "    if tuple(doc[j-2:j+1]) in d:\r\n",
        "      loss -= math.log(d[tuple(doc[j-2:j+1])], 2)\r\n",
        "  score.append(2**(loss/M-2))\r\n",
        "\r\n",
        "# Sort by perplexity and set a cut-off threshold, print the file names (from test_files) and perplexities of the documents above the threshold\r\n",
        "# set cut-off threshold = 2.5, label them as Portuguese\r\n",
        "file_name_score = []\r\n",
        "for i in range(len(test_files)):\r\n",
        "  file_name_score.append([test_files[i], score[i]])\r\n",
        "\r\n",
        "file_score = sorted(file_name_score, key = lambda x: x[1], reverse = True)\r\n",
        "for file in file_score:\r\n",
        "  if file[1] > 2.5:\r\n",
        "    print(file)\r\n",
        "for file in file_score:\r\n",
        "  if file[1] > 2.5:\r\n",
        "    file.append(\"Portuguese\")\r\n",
        "    print(file)\r\n",
        "\r\n"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['br94ma01.txt', 3.0927982899804625]\n",
            "['br94ab02.txt', 3.028269620839364]\n",
            "['ag94ja11.txt', 3.0266510039402093]\n",
            "['br94de01.txt', 3.026526711051293]\n",
            "['br94ju01.txt', 3.0033072696886354]\n",
            "['ag94ou04.txt', 2.9983251181180055]\n",
            "['br94jl01.txt', 2.996986170351466]\n",
            "['br94ag01.txt', 2.977873749170973]\n",
            "['ag94mr1.txt', 2.9670077201424294]\n",
            "['ag94fe1.txt', 2.9661849890702294]\n",
            "['ag94de06.txt', 2.949219423618272]\n",
            "['br94fe1.txt', 2.9391903121718768]\n",
            "['ag94ju07.txt', 2.9391552319546226]\n",
            "['ag94jl12.txt', 2.9280456003106523]\n",
            "['ag94ab12.txt', 2.923502390178443]\n",
            "['br94ja04.txt', 2.9211794800983166]\n",
            "['ag94ma03.txt', 2.9202825789731643]\n",
            "['ag94ag02.txt', 2.9177032466901442]\n",
            "['ag94se06.txt', 2.916070350270325]\n",
            "['ag94no01.txt', 2.907362305167655]\n",
            "['br94ma01.txt', 3.0927982899804625, 'Portuguese']\n",
            "['br94ab02.txt', 3.028269620839364, 'Portuguese']\n",
            "['ag94ja11.txt', 3.0266510039402093, 'Portuguese']\n",
            "['br94de01.txt', 3.026526711051293, 'Portuguese']\n",
            "['br94ju01.txt', 3.0033072696886354, 'Portuguese']\n",
            "['ag94ou04.txt', 2.9983251181180055, 'Portuguese']\n",
            "['br94jl01.txt', 2.996986170351466, 'Portuguese']\n",
            "['br94ag01.txt', 2.977873749170973, 'Portuguese']\n",
            "['ag94mr1.txt', 2.9670077201424294, 'Portuguese']\n",
            "['ag94fe1.txt', 2.9661849890702294, 'Portuguese']\n",
            "['ag94de06.txt', 2.949219423618272, 'Portuguese']\n",
            "['br94fe1.txt', 2.9391903121718768, 'Portuguese']\n",
            "['ag94ju07.txt', 2.9391552319546226, 'Portuguese']\n",
            "['ag94jl12.txt', 2.9280456003106523, 'Portuguese']\n",
            "['ag94ab12.txt', 2.923502390178443, 'Portuguese']\n",
            "['br94ja04.txt', 2.9211794800983166, 'Portuguese']\n",
            "['ag94ma03.txt', 2.9202825789731643, 'Portuguese']\n",
            "['ag94ag02.txt', 2.9177032466901442, 'Portuguese']\n",
            "['ag94se06.txt', 2.916070350270325, 'Portuguese']\n",
            "['ag94no01.txt', 2.907362305167655, 'Portuguese']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQl2u_giVW5e"
      },
      "source": [
        "## 1.3\n",
        "Build a trigram language model with add-λ smoothing (use λ = 0.1).\n",
        "\n",
        "Sort the test documents by perplexity and perform a check for Brazilian Portuguese documents as above:\n",
        "\n",
        "  - Observe the perplexity scores and set a cut-off threshold. All the documents above this threshold score should be categorized as Brazilian Portuguese. \n",
        "  - Print the file names and perplexities of the documents above the threshold\n",
        "\n",
        "  ```\n",
        "      file name, score\n",
        "      file name, score\n",
        "      . . .\n",
        "      file name, score\n",
        "  ```\n",
        "\n",
        "  - Copy this list of filenames and manually annotate them for correctness."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IGUTEk8QUehL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d567fb70-10e4-4df0-9ca2-f696bc0c9c63"
      },
      "source": [
        "# Your code here\r\n",
        "# Build a trigram language model with add-λ smoothing (use λ = 0.1)\r\n",
        "score = []\r\n",
        "for doc in test_doc:\r\n",
        "  loss = 0\r\n",
        "  for i in range(2, len(doc)):\r\n",
        "    if tuple(doc[i-2:i+1]) not in trigram_dict:\r\n",
        "      continue\r\n",
        "    p = (trigram_dict[tuple(doc[i-2:i+1])] + 0.1) / (bigram_dict[tuple(doc[i-2:i])] + 0.1 * len(unigram_dict))\r\n",
        "    loss -= math.log(p, 2)\r\n",
        "  score.append(2**(loss/len(doc)-2))\r\n",
        "\r\n",
        "# Sort the test documents by perplexity. Observe the perplexity scores and set a cut-off threshold.\r\n",
        "# set cut-off threshold = 4, label them as Portuguese\r\n",
        "file_name_score = []\r\n",
        "for i in range(len(test_files)):\r\n",
        "  file_name_score.append([test_files[i], score[i]])\r\n",
        "\r\n",
        "file_score = sorted(file_name_score, key = lambda x: x[1], reverse = True)\r\n",
        "for file in file_score:\r\n",
        "  if file[1] > 4:\r\n",
        "    print(file)\r\n",
        "\r\n",
        "for file in file_score:\r\n",
        "  if file[1] > 4:\r\n",
        "    file.append(\"portuguese\")\r\n",
        "    print(file)\r\n"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['br94ma01.txt', 4.708558765400146]\n",
            "['ag94ja11.txt', 4.535695915049895]\n",
            "['ag94mr1.txt', 4.522396813150476]\n",
            "['br94de01.txt', 4.473970834056714]\n",
            "['ag94ou04.txt', 4.46607394500025]\n",
            "['br94ab02.txt', 4.457455444806291]\n",
            "['br94ag01.txt', 4.453764697290207]\n",
            "['ag94fe1.txt', 4.447371632716189]\n",
            "['br94ju01.txt', 4.438187067144307]\n",
            "['br94jl01.txt', 4.430159927623639]\n",
            "['ag94de06.txt', 4.429197241597126]\n",
            "['ag94ju07.txt', 4.419240086236285]\n",
            "['ag94se06.txt', 4.37705679524243]\n",
            "['ag94jl12.txt', 4.3659380631017815]\n",
            "['ag94ma03.txt', 4.348022038772641]\n",
            "['ag94no01.txt', 4.321492194256158]\n",
            "['ag94ag02.txt', 4.320988220185967]\n",
            "['ag94ab12.txt', 4.313528118956486]\n",
            "['br94fe1.txt', 4.297262402987829]\n",
            "['br94ja04.txt', 4.290974032358516]\n",
            "['br94ma01.txt', 4.708558765400146, 'portuguese']\n",
            "['ag94ja11.txt', 4.535695915049895, 'portuguese']\n",
            "['ag94mr1.txt', 4.522396813150476, 'portuguese']\n",
            "['br94de01.txt', 4.473970834056714, 'portuguese']\n",
            "['ag94ou04.txt', 4.46607394500025, 'portuguese']\n",
            "['br94ab02.txt', 4.457455444806291, 'portuguese']\n",
            "['br94ag01.txt', 4.453764697290207, 'portuguese']\n",
            "['ag94fe1.txt', 4.447371632716189, 'portuguese']\n",
            "['br94ju01.txt', 4.438187067144307, 'portuguese']\n",
            "['br94jl01.txt', 4.430159927623639, 'portuguese']\n",
            "['ag94de06.txt', 4.429197241597126, 'portuguese']\n",
            "['ag94ju07.txt', 4.419240086236285, 'portuguese']\n",
            "['ag94se06.txt', 4.37705679524243, 'portuguese']\n",
            "['ag94jl12.txt', 4.3659380631017815, 'portuguese']\n",
            "['ag94ma03.txt', 4.348022038772641, 'portuguese']\n",
            "['ag94no01.txt', 4.321492194256158, 'portuguese']\n",
            "['ag94ag02.txt', 4.320988220185967, 'portuguese']\n",
            "['ag94ab12.txt', 4.313528118956486, 'portuguese']\n",
            "['br94fe1.txt', 4.297262402987829, 'portuguese']\n",
            "['br94ja04.txt', 4.290974032358516, 'portuguese']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bqhXTB5TXR25"
      },
      "source": [
        "## 1.4\n",
        "Based on your observation from above questions, compare linear interpolation and add-λ smoothing by listing out their pros and cons."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tFq1ECgDI6QG"
      },
      "source": [
        "[Your text here.]  \r\n",
        "Based on my observation from the results of linear interpolation and add-λ smoothing, both the methods can classify the Engilish and Brazilian Portuguese by seting cut-off thresholds. \r\n",
        "The pros of add-λ smoothing: \r\n",
        "1. The model is simpler to build，  \r\n",
        "2. The threshold is easier to find.\r\n",
        "3. We could avoid conditional probility equals to 0.   \r\n",
        "\r\n",
        "The cons of add-λ smoothing:  \r\n",
        "1. The model is not used in N-grams.\r\n",
        "2. Only for text classification.\r\n",
        "3. The zero counts are not too many.\r\n",
        "\r\n",
        "Then pros of linear interpolation:\r\n",
        "1. The model can be used in N-grams.\r\n",
        "2. The model can be trained.\r\n",
        "3. Most times linear interpolation works better.\r\n",
        "4. Easy to use\r\n",
        "\r\n",
        "The cons of linear interpolation: \r\n",
        "1. The model is not very accurate for non-linear.\r\n",
        "2. The accuracy is not good as that of add-λ smoothing.\r\n",
        "3. The threshold is harder to find."
      ]
    }
  ]
}